<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Using R for Introduction to Econometrics</title>
  <meta name="description" content="Using <tt>R</tt> for Introduction to Econometrics">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Using R for Introduction to Econometrics" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="Emwikts1970/URFITE-Bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Using R for Introduction to Econometrics" />
  
  
  

<meta name="author" content="Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer">


<meta name="date" content="2018-07-20">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="a-general-strategy-for-modelling-nonlinear-regression-functions.html">
<link rel="next" href="interactions-between-independent-variables.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>
<script src="js/hideOutput.js"></script>
<script type="text/javascript" src="MathJax-2.7.2/MathJax.js?config=default"></script>

 <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js", "TeX/AMSmath.js"],
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        jax: ["input/TeX","output/CommonHTML"]
      });
      MathJax.Hub.processSectionDelay = 0;
  </script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110299877-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110299877-1');
</script>

<!-- open review block -->

<script async defer src="https://hypothes.is/embed.js"></script>

<!-- datacamp-light-latest -->

<script src="https://cdn.datacamp.com/datacamp-light-latest.min.js"></script>


<!-- <script src="js/d3.v3.min.js"></script>
<script src="js/leastsquares.js"></script>
<script src="js/d3functions.js"></script>
<script src="d3.slider.js"></script>
<script src="slrm.js"></script> -->


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">URFITE</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="a-very-short-introduction-to-r-and-rstudio.html"><a href="a-very-short-introduction-to-r-and-rstudio.html"><i class="fa fa-check"></i><b>1.1</b> A Very Short Introduction to <tt>R</tt> and <em>RStudio</em></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="pt.html"><a href="pt.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.1" data-path="random-variables-and-probability-distributions.html"><a href="random-variables-and-probability-distributions.html"><i class="fa fa-check"></i><b>2.1</b> Random Variables and Probability Distributions</a><ul>
<li class="chapter" data-level="" data-path="random-variables-and-probability-distributions.html"><a href="random-variables-and-probability-distributions.html#probability-distributions-of-discrete-random-variables"><i class="fa fa-check"></i>Probability Distributions of Discrete Random Variables</a></li>
<li class="chapter" data-level="" data-path="random-variables-and-probability-distributions.html"><a href="random-variables-and-probability-distributions.html#bernoulli-trials"><i class="fa fa-check"></i>Bernoulli Trials</a></li>
<li class="chapter" data-level="" data-path="random-variables-and-probability-distributions.html"><a href="random-variables-and-probability-distributions.html#expected-value-mean-and-variance"><i class="fa fa-check"></i>Expected Value, Mean and Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="probability-distributions-of-continuous-random-variables.html"><a href="probability-distributions-of-continuous-random-variables.html"><i class="fa fa-check"></i><b>2.2</b> Probability Distributions of Continuous Random Variables</a><ul>
<li class="chapter" data-level="" data-path="probability-distributions-of-continuous-random-variables.html"><a href="probability-distributions-of-continuous-random-variables.html#the-normal-distribution"><i class="fa fa-check"></i>The Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="probability-distributions-of-continuous-random-variables.html"><a href="probability-distributions-of-continuous-random-variables.html#the-chi-squared-distribution"><i class="fa fa-check"></i>The Chi-Squared Distribution</a></li>
<li class="chapter" data-level="" data-path="probability-distributions-of-continuous-random-variables.html"><a href="probability-distributions-of-continuous-random-variables.html#thetdist"><i class="fa fa-check"></i>The Student t Distribution</a></li>
<li class="chapter" data-level="" data-path="probability-distributions-of-continuous-random-variables.html"><a href="probability-distributions-of-continuous-random-variables.html#the-f-distribution"><i class="fa fa-check"></i>The F Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="RSATDOSA.html"><a href="RSATDOSA.html"><i class="fa fa-check"></i><b>2.3</b> Random Sampling and the Distribution of Sample Averages</a><ul>
<li class="chapter" data-level="" data-path="RSATDOSA.html"><a href="RSATDOSA.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i>Mean and Variance of the Sample Mean</a></li>
<li class="chapter" data-level="" data-path="RSATDOSA.html"><a href="RSATDOSA.html#large-sample-approximations-to-sampling-distributions"><i class="fa fa-check"></i>Large Sample Approximations to Sampling Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="arosur.html"><a href="arosur.html"><i class="fa fa-check"></i><b>3</b> A Review of Statistics using R</a><ul>
<li class="chapter" data-level="3.1" data-path="estimation-of-the-population-mean.html"><a href="estimation-of-the-population-mean.html"><i class="fa fa-check"></i><b>3.1</b> Estimation of the Population Mean</a></li>
<li class="chapter" data-level="3.2" data-path="potsm.html"><a href="potsm.html"><i class="fa fa-check"></i><b>3.2</b> Properties of the Sample Mean</a></li>
<li class="chapter" data-level="3.3" data-path="hypothesis-tests-concerning-the-population-mean.html"><a href="hypothesis-tests-concerning-the-population-mean.html"><i class="fa fa-check"></i><b>3.3</b> Hypothesis Tests Concerning the Population Mean</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-concerning-the-population-mean.html"><a href="hypothesis-tests-concerning-the-population-mean.html#the-p-value"><i class="fa fa-check"></i>The p-Value</a></li>
<li><a href="hypothesis-tests-concerning-the-population-mean.html#calculating-the-p-value-when-sigma_y-is-known">Calculating the <span class="math inline">\(p\)</span>-Value When <span class="math inline">\(\sigma_Y\)</span> Is Known</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-concerning-the-population-mean.html"><a href="hypothesis-tests-concerning-the-population-mean.html#SVSSDASE"><i class="fa fa-check"></i>Sample Variance, Sample Standard Deviation and Standard Error</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-concerning-the-population-mean.html"><a href="hypothesis-tests-concerning-the-population-mean.html#calculating-the-p-value-when-the-standard-deviation-is-unknown"><i class="fa fa-check"></i>Calculating the p-value When the Standard Deviation is Unknown</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-concerning-the-population-mean.html"><a href="hypothesis-tests-concerning-the-population-mean.html#the-t-statistic"><i class="fa fa-check"></i>The t-statistic</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-concerning-the-population-mean.html"><a href="hypothesis-tests-concerning-the-population-mean.html#hypothesis-testing-with-a-prespecified-significance-level"><i class="fa fa-check"></i>Hypothesis Testing with a Prespecified Significance Level</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-concerning-the-population-mean.html"><a href="hypothesis-tests-concerning-the-population-mean.html#one-sided-alternatives"><i class="fa fa-check"></i>One-sided Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="confidence-intervals-for-the-population-mean.html"><a href="confidence-intervals-for-the-population-mean.html"><i class="fa fa-check"></i><b>3.4</b> Confidence Intervals for the Population Mean</a></li>
<li class="chapter" data-level="3.5" data-path="comparing-means-from-different-populations.html"><a href="comparing-means-from-different-populations.html"><i class="fa fa-check"></i><b>3.5</b> Comparing Means from Different Populations</a></li>
<li class="chapter" data-level="3.6" data-path="aattggoe.html"><a href="aattggoe.html"><i class="fa fa-check"></i><b>3.6</b> An Application to the Gender Gap of Earnings</a></li>
<li class="chapter" data-level="3.7" data-path="scatterplots-sample-covariance-and-sample-correlation.html"><a href="scatterplots-sample-covariance-and-sample-correlation.html"><i class="fa fa-check"></i><b>3.7</b> Scatterplots, Sample Covariance and Sample Correlation</a></li>
<li class="chapter" data-level="3.8" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lrwor.html"><a href="lrwor.html"><i class="fa fa-check"></i><b>4</b> Linear Regression with One Regressor</a><ul>
<li class="chapter" data-level="4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>4.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="4.2" data-path="estimating-the-coefficients-of-the-linear-regression-model.html"><a href="estimating-the-coefficients-of-the-linear-regression-model.html"><i class="fa fa-check"></i><b>4.2</b> Estimating the Coefficients of the Linear Regression Model</a><ul>
<li class="chapter" data-level="" data-path="estimating-the-coefficients-of-the-linear-regression-model.html"><a href="estimating-the-coefficients-of-the-linear-regression-model.html#the-ordinary-least-squares-estimator"><i class="fa fa-check"></i>The Ordinary Least Squares Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="measures-of-fit.html"><a href="measures-of-fit.html"><i class="fa fa-check"></i><b>4.3</b> Measures of Fit</a><ul>
<li class="chapter" data-level="" data-path="measures-of-fit.html"><a href="measures-of-fit.html#the-coefficient-of-determination"><i class="fa fa-check"></i>The Coefficient of Determination</a></li>
<li class="chapter" data-level="" data-path="measures-of-fit.html"><a href="measures-of-fit.html#the-standard-error-of-the-regression"><i class="fa fa-check"></i>The Standard Error of the Regression</a></li>
<li class="chapter" data-level="" data-path="measures-of-fit.html"><a href="measures-of-fit.html#application-to-the-test-score-data"><i class="fa fa-check"></i>Application to the Test Score Data</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tlsa.html"><a href="tlsa.html"><i class="fa fa-check"></i><b>4.4</b> The Least Squares Assumptions</a><ul>
<li class="chapter" data-level="" data-path="tlsa.html"><a href="tlsa.html#assumption-1-the-error-term-has-conditional-mean-of-zero"><i class="fa fa-check"></i>Assumption #1: The Error Term has Conditional Mean of Zero</a></li>
<li class="chapter" data-level="" data-path="tlsa.html"><a href="tlsa.html#assumption-2-independently-and-identically-distributed-data"><i class="fa fa-check"></i>Assumption #2: Independently and Identically Distributed Data</a></li>
<li class="chapter" data-level="" data-path="tlsa.html"><a href="tlsa.html#assumption-3-large-outliers-are-unlikely"><i class="fa fa-check"></i>Assumption #3: Large Outliers are Unlikely</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="tsdotoe.html"><a href="tsdotoe.html"><i class="fa fa-check"></i><b>4.5</b> The Sampling Distribution of the OLS Estimator</a><ul>
<li class="chapter" data-level="" data-path="tsdotoe.html"><a href="tsdotoe.html#simulation-study-1"><i class="fa fa-check"></i>Simulation Study 1</a></li>
<li class="chapter" data-level="" data-path="tsdotoe.html"><a href="tsdotoe.html#simulation-study-2"><i class="fa fa-check"></i>Simulation Study 2</a></li>
<li class="chapter" data-level="" data-path="tsdotoe.html"><a href="tsdotoe.html#simulation-study-3"><i class="fa fa-check"></i>Simulation Study 3</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="htaciitslrm.html"><a href="htaciitslrm.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model</a><ul>
<li class="chapter" data-level="5.1" data-path="testing-two-sided-hypotheses-concerning-the-slope-coefficient.html"><a href="testing-two-sided-hypotheses-concerning-the-slope-coefficient.html"><i class="fa fa-check"></i><b>5.1</b> Testing Two-Sided Hypotheses Concerning the Slope Coefficient</a></li>
<li class="chapter" data-level="5.2" data-path="cifrc.html"><a href="cifrc.html"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals for Regression Coefficients</a><ul>
<li class="chapter" data-level="" data-path="cifrc.html"><a href="cifrc.html#simulation-study-confidence-intervals"><i class="fa fa-check"></i>Simulation Study: Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="rwxiabv.html"><a href="rwxiabv.html"><i class="fa fa-check"></i><b>5.3</b> Regression when X is a Binary Variable</a></li>
<li class="chapter" data-level="5.4" data-path="heteroskedasticity-and-homoskedasticity.html"><a href="heteroskedasticity-and-homoskedasticity.html"><i class="fa fa-check"></i><b>5.4</b> Heteroskedasticity and Homoskedasticity</a><ul>
<li class="chapter" data-level="" data-path="heteroskedasticity-and-homoskedasticity.html"><a href="heteroskedasticity-and-homoskedasticity.html#a-real-world-example-for-heteroskedasticity"><i class="fa fa-check"></i>A Real-World Example for Heteroskedasticity</a></li>
<li class="chapter" data-level="" data-path="heteroskedasticity-and-homoskedasticity.html"><a href="heteroskedasticity-and-homoskedasticity.html#should-we-care-about-heteroskedasticity"><i class="fa fa-check"></i>Should We Care About Heteroskedasticity?</a></li>
<li class="chapter" data-level="" data-path="heteroskedasticity-and-homoskedasticity.html"><a href="heteroskedasticity-and-homoskedasticity.html#computation-of-heteroskedasticity-robust-standard-errors"><i class="fa fa-check"></i>Computation of Heteroskedasticity-Robust Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="the-gauss-markov-theorem.html"><a href="the-gauss-markov-theorem.html"><i class="fa fa-check"></i><b>5.5</b> The Gauss-Markov Theorem</a><ul>
<li class="chapter" data-level="" data-path="the-gauss-markov-theorem.html"><a href="the-gauss-markov-theorem.html#simulation-study-blue-estimator"><i class="fa fa-check"></i>Simulation Study: BLUE Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="using-the-t-statistic-in-regression-when-the-sample-size-is-small.html"><a href="using-the-t-statistic-in-regression-when-the-sample-size-is-small.html"><i class="fa fa-check"></i><b>5.6</b> Using the t-Statistic in Regression When the Sample Size Is Small</a></li>
<li class="chapter" data-level="5.7" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="rmwmr.html"><a href="rmwmr.html"><i class="fa fa-check"></i><b>6</b> Regression Models with Multiple Regressors</a><ul>
<li class="chapter" data-level="6.1" data-path="omitted-variable-bias.html"><a href="omitted-variable-bias.html"><i class="fa fa-check"></i><b>6.1</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="6.2" data-path="the-multiple-regression-model.html"><a href="the-multiple-regression-model.html"><i class="fa fa-check"></i><b>6.2</b> The Multiple Regression Model</a></li>
<li class="chapter" data-level="6.3" data-path="mofimr.html"><a href="mofimr.html"><i class="fa fa-check"></i><b>6.3</b> Measures of Fit in Multiple Regression</a></li>
<li class="chapter" data-level="6.4" data-path="ols-assumptions-in-multiple-regression.html"><a href="ols-assumptions-in-multiple-regression.html"><i class="fa fa-check"></i><b>6.4</b> OLS Assumptions in Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="ols-assumptions-in-multiple-regression.html"><a href="ols-assumptions-in-multiple-regression.html#multicollinearity"><i class="fa fa-check"></i>Multicollinearity</a></li>
<li class="chapter" data-level="" data-path="ols-assumptions-in-multiple-regression.html"><a href="ols-assumptions-in-multiple-regression.html#simulation-study-imperfect-multicollinearity"><i class="fa fa-check"></i>Simulation Study: Imperfect Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="the-distribution-of-the-ols-estimators-in-multiple-regression.html"><a href="the-distribution-of-the-ols-estimators-in-multiple-regression.html"><i class="fa fa-check"></i><b>6.5</b> The Distribution of the OLS Estimators in Multiple Regression</a></li>
<li class="chapter" data-level="6.6" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="htaciimr.html"><a href="htaciimr.html"><i class="fa fa-check"></i><b>7</b> Hypothesis Tests and Confidence Intervals in Multiple Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="hypothesis-tests-and-confidence-intervals-for-a-single-coefficient.html"><a href="hypothesis-tests-and-confidence-intervals-for-a-single-coefficient.html"><i class="fa fa-check"></i><b>7.1</b> Hypothesis Tests and Confidence Intervals for a Single Coefficient</a></li>
<li class="chapter" data-level="7.2" data-path="an-application-to-test-scores-and-the-student-teacher-ratio.html"><a href="an-application-to-test-scores-and-the-student-teacher-ratio.html"><i class="fa fa-check"></i><b>7.2</b> An Application to Test Scores and the Student-Teacher Ratio</a><ul>
<li class="chapter" data-level="" data-path="an-application-to-test-scores-and-the-student-teacher-ratio.html"><a href="an-application-to-test-scores-and-the-student-teacher-ratio.html#another-augmentation-of-the-model"><i class="fa fa-check"></i>Another Augmentation of the Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="joint-hypothesis-testing-using-the-f-statistic.html"><a href="joint-hypothesis-testing-using-the-f-statistic.html"><i class="fa fa-check"></i><b>7.3</b> Joint Hypothesis Testing Using the <span class="math inline">\(F\)</span>-Statistic</a></li>
<li class="chapter" data-level="7.4" data-path="confidence-sets-for-multiple-coefficients.html"><a href="confidence-sets-for-multiple-coefficients.html"><i class="fa fa-check"></i><b>7.4</b> Confidence Sets for Multiple Coefficients</a></li>
<li class="chapter" data-level="7.5" data-path="model-specification-for-multiple-regression.html"><a href="model-specification-for-multiple-regression.html"><i class="fa fa-check"></i><b>7.5</b> Model Specification for Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="model-specification-for-multiple-regression.html"><a href="model-specification-for-multiple-regression.html#model-specification-in-theory-and-in-practice"><i class="fa fa-check"></i>Model Specification in Theory and in Practice</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="analysis-of-the-test-score-data-set.html"><a href="analysis-of-the-test-score-data-set.html"><i class="fa fa-check"></i><b>7.6</b> Analysis of the Test Score Data Set</a></li>
<li class="chapter" data-level="7.7" data-path="exercises-5.html"><a href="exercises-5.html"><i class="fa fa-check"></i><b>7.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nrf.html"><a href="nrf.html"><i class="fa fa-check"></i><b>8</b> Nonlinear Regression Functions</a><ul>
<li class="chapter" data-level="8.1" data-path="a-general-strategy-for-modelling-nonlinear-regression-functions.html"><a href="a-general-strategy-for-modelling-nonlinear-regression-functions.html"><i class="fa fa-check"></i><b>8.1</b> A General Strategy for Modelling Nonlinear Regression Functions</a></li>
<li class="chapter" data-level="8.2" data-path="nfoasiv.html"><a href="nfoasiv.html"><i class="fa fa-check"></i><b>8.2</b> Nonlinear Functions of a Single Independent Variable</a><ul>
<li class="chapter" data-level="" data-path="nfoasiv.html"><a href="nfoasiv.html#polynomials"><i class="fa fa-check"></i>Polynomials</a></li>
<li class="chapter" data-level="" data-path="nfoasiv.html"><a href="nfoasiv.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="interactions-between-independent-variables.html"><a href="interactions-between-independent-variables.html"><i class="fa fa-check"></i><b>8.3</b> Interactions Between Independent Variables</a></li>
<li class="chapter" data-level="8.4" data-path="nonlinear-effects-on-test-scores-of-the-student-teacher-ratio.html"><a href="nonlinear-effects-on-test-scores-of-the-student-teacher-ratio.html"><i class="fa fa-check"></i><b>8.4</b> Nonlinear Effects on Test Scores of the Student-Teacher Ratio</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="asbomr.html"><a href="asbomr.html"><i class="fa fa-check"></i><b>9</b> Assessing Studies Based on Multiple Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="internal-and-external-validity.html"><a href="internal-and-external-validity.html"><i class="fa fa-check"></i><b>9.1</b> Internal and External Validity</a></li>
<li class="chapter" data-level="9.2" data-path="threats-to-internal-validity-of-multiple-regression-analysis.html"><a href="threats-to-internal-validity-of-multiple-regression-analysis.html"><i class="fa fa-check"></i><b>9.2</b> Threats to Internal Validity of Multiple Regression Analysis</a></li>
<li class="chapter" data-level="9.3" data-path="internal-and-external-validity-when-the-regression-is-used-for-forecasting.html"><a href="internal-and-external-validity-when-the-regression-is-used-for-forecasting.html"><i class="fa fa-check"></i><b>9.3</b> Internal and External Validity When the Regression is Used for Forecasting</a></li>
<li class="chapter" data-level="9.4" data-path="etsacs.html"><a href="etsacs.html"><i class="fa fa-check"></i><b>9.4</b> Example: Test Scores and Class Size</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="rwpd.html"><a href="rwpd.html"><i class="fa fa-check"></i><b>10</b> Regression with Panel Data</a><ul>
<li class="chapter" data-level="10.1" data-path="panel-data.html"><a href="panel-data.html"><i class="fa fa-check"></i><b>10.1</b> Panel Data</a></li>
<li class="chapter" data-level="10.2" data-path="PDWTTP.html"><a href="PDWTTP.html"><i class="fa fa-check"></i><b>10.2</b> Panel Data with Two Time Periods: “Before and After” Comparisons</a></li>
<li class="chapter" data-level="10.3" data-path="fixed-effects-regression.html"><a href="fixed-effects-regression.html"><i class="fa fa-check"></i><b>10.3</b> Fixed Effects Regression</a><ul>
<li class="chapter" data-level="" data-path="fixed-effects-regression.html"><a href="fixed-effects-regression.html#estimation-and-inference"><i class="fa fa-check"></i>Estimation and Inference</a></li>
<li class="chapter" data-level="" data-path="fixed-effects-regression.html"><a href="fixed-effects-regression.html#application-to-traffic-deaths"><i class="fa fa-check"></i>Application to Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="regression-with-time-fixed-effects.html"><a href="regression-with-time-fixed-effects.html"><i class="fa fa-check"></i><b>10.4</b> Regression with Time Fixed Effects</a></li>
<li class="chapter" data-level="10.5" data-path="tferaaseffer.html"><a href="tferaaseffer.html"><i class="fa fa-check"></i><b>10.5</b> The Fixed Effects Regression Assumptions and Standard Errors for Fixed Effects Regression</a></li>
<li class="chapter" data-level="10.6" data-path="drunk-driving-laws-and-traffic-deaths.html"><a href="drunk-driving-laws-and-traffic-deaths.html"><i class="fa fa-check"></i><b>10.6</b> Drunk Driving Laws and Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="rwabdv.html"><a href="rwabdv.html"><i class="fa fa-check"></i><b>11</b> Regression with a Binary Dependent Variable</a><ul>
<li class="chapter" data-level="11.1" data-path="binary-dependent-variables-and-the-linear-probability-model.html"><a href="binary-dependent-variables-and-the-linear-probability-model.html"><i class="fa fa-check"></i><b>11.1</b> Binary Dependent Variables and the Linear Probability Model</a></li>
<li class="chapter" data-level="11.2" data-path="probit-and-logit-regression.html"><a href="probit-and-logit-regression.html"><i class="fa fa-check"></i><b>11.2</b> Probit and Logit Regression</a><ul>
<li class="chapter" data-level="" data-path="probit-and-logit-regression.html"><a href="probit-and-logit-regression.html#probit-regression"><i class="fa fa-check"></i>Probit Regression</a></li>
<li class="chapter" data-level="" data-path="probit-and-logit-regression.html"><a href="probit-and-logit-regression.html#logit-regression"><i class="fa fa-check"></i>Logit Regression</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="estimation-and-inference-in-the-logit-and-probit-models.html"><a href="estimation-and-inference-in-the-logit-and-probit-models.html"><i class="fa fa-check"></i><b>11.3</b> Estimation and Inference in the Logit and Probit Models</a></li>
<li class="chapter" data-level="11.4" data-path="application-to-the-boston-hmda-data.html"><a href="application-to-the-boston-hmda-data.html"><i class="fa fa-check"></i><b>11.4</b> Application to the Boston HMDA Data</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ivr.html"><a href="ivr.html"><i class="fa fa-check"></i><b>12</b> Instrumental Variables Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="TIVEWASRAASI.html"><a href="TIVEWASRAASI.html"><i class="fa fa-check"></i><b>12.1</b> The IV Estimator with a Single Regressor and a Single Instrument</a></li>
<li class="chapter" data-level="12.2" data-path="TGIVRM.html"><a href="TGIVRM.html"><i class="fa fa-check"></i><b>12.2</b> The General IV Regression Model</a></li>
<li class="chapter" data-level="12.3" data-path="civ.html"><a href="civ.html"><i class="fa fa-check"></i><b>12.3</b> Checking Instrument Validity</a></li>
<li class="chapter" data-level="12.4" data-path="attdfc.html"><a href="attdfc.html"><i class="fa fa-check"></i><b>12.4</b> Application to the Demand for Cigarettes</a></li>
<li class="chapter" data-level="12.5" data-path="where-do-valid-instruments-come-from.html"><a href="where-do-valid-instruments-come-from.html"><i class="fa fa-check"></i><b>12.5</b> Where Do Valid Instruments Come From?</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="eaqe.html"><a href="eaqe.html"><i class="fa fa-check"></i><b>13</b> Experiments and Quasi-Experiments</a><ul>
<li class="chapter" data-level="13.1" data-path="potential-outcomes-causal-effects-and-idealized-experiments.html"><a href="potential-outcomes-causal-effects-and-idealized-experiments.html"><i class="fa fa-check"></i><b>13.1</b> Potential Outcomes, Causal Effects and Idealized Experiments</a></li>
<li class="chapter" data-level="13.2" data-path="threats-to-validity-of-experiments.html"><a href="threats-to-validity-of-experiments.html"><i class="fa fa-check"></i><b>13.2</b> Threats to Validity of Experiments</a></li>
<li class="chapter" data-level="13.3" data-path="experimental-estimates-of-the-effect-of-class-size-reductions.html"><a href="experimental-estimates-of-the-effect-of-class-size-reductions.html"><i class="fa fa-check"></i><b>13.3</b> Experimental Estimates of the Effect of Class Size Reductions</a><ul>
<li class="chapter" data-level="" data-path="experimental-estimates-of-the-effect-of-class-size-reductions.html"><a href="experimental-estimates-of-the-effect-of-class-size-reductions.html#experimental-design-and-the-data-set"><i class="fa fa-check"></i>Experimental Design and the Data Set</a></li>
<li class="chapter" data-level="" data-path="experimental-estimates-of-the-effect-of-class-size-reductions.html"><a href="experimental-estimates-of-the-effect-of-class-size-reductions.html#analysis-of-the-star-data"><i class="fa fa-check"></i>Analysis of the STAR Data</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="quasi-experiments.html"><a href="quasi-experiments.html"><i class="fa fa-check"></i><b>13.4</b> Quasi Experiments</a><ul>
<li class="chapter" data-level="" data-path="quasi-experiments.html"><a href="quasi-experiments.html#the-differences-in-differences-estimator"><i class="fa fa-check"></i>The Differences-in-Differences Estimator</a></li>
<li class="chapter" data-level="" data-path="quasi-experiments.html"><a href="quasi-experiments.html#regression-discontinuity-estimators"><i class="fa fa-check"></i>Regression Discontinuity Estimators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ittsraf.html"><a href="ittsraf.html"><i class="fa fa-check"></i><b>14</b> Introduction to Time Series Regression and Forecasting</a><ul>
<li class="chapter" data-level="14.1" data-path="using-regression-models-for-forecasting.html"><a href="using-regression-models-for-forecasting.html"><i class="fa fa-check"></i><b>14.1</b> Using Regression Models for Forecasting</a></li>
<li class="chapter" data-level="14.2" data-path="tsdasc.html"><a href="tsdasc.html"><i class="fa fa-check"></i><b>14.2</b> Time Series Data and Serial Correlation</a><ul>
<li class="chapter" data-level="" data-path="tsdasc.html"><a href="tsdasc.html#notation-lags-differences-logarithms-and-growth-rates"><i class="fa fa-check"></i>Notation, Lags, Differences, Logarithms and Growth Rates</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="autoregressions.html"><a href="autoregressions.html"><i class="fa fa-check"></i><b>14.3</b> Autoregressions</a><ul>
<li class="chapter" data-level="" data-path="autoregressions.html"><a href="autoregressions.html#autoregressive-models-of-order-p"><i class="fa fa-check"></i>Autoregressive Models of Order p</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="cybtmpi.html"><a href="cybtmpi.html"><i class="fa fa-check"></i><b>14.4</b> Can You Beat the Market? (Part I)</a></li>
<li class="chapter" data-level="14.5" data-path="apatadlm.html"><a href="apatadlm.html"><i class="fa fa-check"></i><b>14.5</b> Additional Predictors and The ADL Model</a><ul>
<li class="chapter" data-level="" data-path="apatadlm.html"><a href="apatadlm.html#forecast-uncertainty-and-forecast-intervals"><i class="fa fa-check"></i>Forecast Uncertainty and Forecast Intervals</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="llsuic.html"><a href="llsuic.html"><i class="fa fa-check"></i><b>14.6</b> Lag Length Selection Using Information Criteria</a></li>
<li class="chapter" data-level="14.7" data-path="nit.html"><a href="nit.html"><i class="fa fa-check"></i><b>14.7</b> Nonstationarity I: Trends</a></li>
<li class="chapter" data-level="14.8" data-path="niib.html"><a href="niib.html"><i class="fa fa-check"></i><b>14.8</b> Nonstationarity II: Breaks</a></li>
<li class="chapter" data-level="14.9" data-path="can-you-beat-the-market-part-ii.html"><a href="can-you-beat-the-market-part-ii.html"><i class="fa fa-check"></i><b>14.9</b> Can You Beat the Market? (Part II)</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="eodce.html"><a href="eodce.html"><i class="fa fa-check"></i><b>15</b> Estimation of Dynamic Causal Effects</a><ul>
<li class="chapter" data-level="15.1" data-path="the-orange-juice-data.html"><a href="the-orange-juice-data.html"><i class="fa fa-check"></i><b>15.1</b> The Orange Juice Data</a></li>
<li class="chapter" data-level="15.2" data-path="dynamic-causal-effects.html"><a href="dynamic-causal-effects.html"><i class="fa fa-check"></i><b>15.2</b> Dynamic Causal Effects</a></li>
<li class="chapter" data-level="15.3" data-path="dynamic-multipliers-and-cumulative-dynamic-multipliers.html"><a href="dynamic-multipliers-and-cumulative-dynamic-multipliers.html"><i class="fa fa-check"></i><b>15.3</b> Dynamic Multipliers and Cumulative Dynamic Multipliers</a></li>
<li class="chapter" data-level="15.4" data-path="hac-standard-errors.html"><a href="hac-standard-errors.html"><i class="fa fa-check"></i><b>15.4</b> HAC Standard Errors</a></li>
<li class="chapter" data-level="15.5" data-path="estimation-of-dynamic-causal-effects-with-strictly-exogeneous-regressors.html"><a href="estimation-of-dynamic-causal-effects-with-strictly-exogeneous-regressors.html"><i class="fa fa-check"></i><b>15.5</b> Estimation of Dynamic Causal Effects with Strictly Exogeneous Regressors</a></li>
<li class="chapter" data-level="15.6" data-path="orange-juice-prices-and-cold-weather.html"><a href="orange-juice-prices-and-cold-weather.html"><i class="fa fa-check"></i><b>15.6</b> Orange Juice Prices and Cold Weather</a></li>
<li class="chapter" data-level="15.7" data-path="summary-7.html"><a href="summary-7.html"><i class="fa fa-check"></i><b>15.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="atitsr.html"><a href="atitsr.html"><i class="fa fa-check"></i><b>16</b> Additional Topics in Time Series Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="vector-autoregressions.html"><a href="vector-autoregressions.html"><i class="fa fa-check"></i><b>16.1</b> Vector Autoregressions</a></li>
<li class="chapter" data-level="16.2" data-path="ooiatdfglsurt.html"><a href="ooiatdfglsurt.html"><i class="fa fa-check"></i><b>16.2</b> Orders of Integration and the DF-GLS Unit Root Test</a></li>
<li class="chapter" data-level="16.3" data-path="cointegration.html"><a href="cointegration.html"><i class="fa fa-check"></i><b>16.3</b> Cointegration</a></li>
<li class="chapter" data-level="16.4" data-path="volatility-clustering-and-autoregressive-conditional-heteroskedasticity.html"><a href="volatility-clustering-and-autoregressive-conditional-heteroskedasticity.html"><i class="fa fa-check"></i><b>16.4</b> Volatility Clustering and Autoregressive Conditional Heteroskedasticity</a><ul>
<li class="chapter" data-level="" data-path="volatility-clustering-and-autoregressive-conditional-heteroskedasticity.html"><a href="volatility-clustering-and-autoregressive-conditional-heteroskedasticity.html#arch-and-garch-models"><i class="fa fa-check"></i>ARCH and GARCH Models</a></li>
<li class="chapter" data-level="" data-path="volatility-clustering-and-autoregressive-conditional-heteroskedasticity.html"><a href="volatility-clustering-and-autoregressive-conditional-heteroskedasticity.html#application-to-stock-price-volatility"><i class="fa fa-check"></i>Application to Stock Price Volatility</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="summary-8.html"><a href="summary-8.html"><i class="fa fa-check"></i><b>16.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Using <tt>R</tt> for Introduction to Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div class = rmdreview>
This book is in <b>Open Review</b>. We want your feedback to make the book better for you and other students. You may annotate some text by <span style="background-color: #3297FD; color: white">selecting it with the cursor</span> and then click the <i class="h-icon-annotate"></i> on the pop-up menu. You can also see the annotations of others: click the <i class="h-icon-chevron-left"></i> in the upper right hand corner of the page <i class="fa fa-arrow-circle-right  fa-rotate-315" aria-hidden="true"></i>
</div>
<div id="nfoasiv" class="section level2">
<h2><span class="header-section-number">8.2</span> Nonlinear Functions of a Single Independent Variable</h2>
<div id="polynomials" class="section level3 unnumbered">
<h3>Polynomials</h3>
<p>The approach used to obtain a quadratic model can be generalized to polynomial models of arbitrary degree <span class="math inline">\(r\)</span>. <span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + \beta_2 X_i^2 + \ldots + \beta_r X_i^r + u_i\]</span></p>
<p>A cubic model (<span class="math inline">\(r=3\)</span>) for instance can be estimated in the same way as the quadratic model; we just have to add <tt>I(income^3)</tt> to the <tt>formula</tt> argument in our call of <tt>lm()</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cubic_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>income <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(income<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(income<span class="op">^</span><span class="dv">3</span>), <span class="dt">data =</span> CASchools)</code></pre></div>
<p>In practice the question will arise which polynomial order should be chosen. First note that, similarly as for <span class="math inline">\(r=2\)</span>, we can test the null hypothesis that the true relation is linear against the alternative hypothesis that the relationship is a polynomial of degree <span class="math inline">\(r\)</span>:</p>
<p><span class="math display">\[ H_0: \beta_2=0, \ \beta_3=0,\dots,\beta_r=0 \ \ \ \text{vs.} \ \ \ H_1: \text{at least one} \ \beta_j\neq0, \ j=2,\dots,r \]</span></p>
<p>This is a joint null hypothesis with <span class="math inline">\(r-1\)</span> restrictions so it can be tested using the <span class="math inline">\(F\)</span>-test presented in Chapter <a href="htaciimr.html#htaciimr">7</a>. Remember that the function <tt>linearHypothesis()</tt> can used to conduct such tests. If, for example, we would like to test the null hypothesis of a linear model against the alternative of a polynomial of a maximal degree <span class="math inline">\(r=3\)</span>, we could simply do the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># test the hypothesis</span>
<span class="kw">linearHypothesis</span>(cubic_model, 
                 <span class="kw">c</span>(<span class="st">&quot;I(income^2)=0&quot;</span>, <span class="st">&quot;I(income^3)=0&quot;</span>),
                 <span class="dt">white.adj =</span> <span class="st">&quot;hc1&quot;</span>
)</code></pre></div>
<pre><code>## Linear hypothesis test
## 
## Hypothesis:
## I(income^2) = 0
## I(income^3) = 0
## 
## Model 1: restricted model
## Model 2: score ~ income + I(income^2) + I(income^3)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F    Pr(&gt;F)    
## 1    418                        
## 2    416  2 37.691 9.043e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The <span class="math inline">\(p\)</span>-value for this test is very small so that we reject the null hypothesis. However, this does not tell us <em>which</em> <span class="math inline">\(r\)</span> to choose. In practice, one approach to determine the degree of the polynomial is to use <em>sequential testing</em>:</p>
<ol style="list-style-type: decimal">
<li>Estimate a polynomial model for some maximum value <span class="math inline">\(r\)</span>.</li>
<li>Use a <span class="math inline">\(t\)</span>-test to test whether <span class="math inline">\(\beta_r = 0\)</span>. <b>Rejection</b> of the null means that <span class="math inline">\(X^r\)</span> belongs in the regression equation.</li>
<li><b>Acceptance</b> of the null in step 2 means that <span class="math inline">\(X^r\)</span> can be eliminated from the model. Continue by repeating step 1 with order <span class="math inline">\(r-1\)</span> and test whether <span class="math inline">\(\beta_{r-1}=0\)</span>. If the test rejects, use a polynomial model of order <span class="math inline">\(r-1\)</span>.</li>
<li>If the tests from step 3 rejects, continue with the procedure until the coefficient on the highest power is statistically significant.</li>
</ol>
<p>There is no unambiguous guideline how to choose <span class="math inline">\(r\)</span> in step one. However, as pointed out in <span class="citation">J. Stock &amp; Watson (<a href="#ref-stock2015">2015</a>)</span>, economic data is often smooth such that it is appropriate to choose small orders like <span class="math inline">\(2\)</span>, <span class="math inline">\(3\)</span>, or <span class="math inline">\(4\)</span>.</p>
<p>We will now demonstrate how to apply <it>sequential testing</it> by the example of the cubic model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(cubic_model)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ income + I(income^2) + I(income^3), data = CASchools)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -44.28  -9.21   0.20   8.32  31.16 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  6.001e+02  5.830e+00 102.937  &lt; 2e-16 ***
## income       5.019e+00  8.595e-01   5.839 1.06e-08 ***
## I(income^2) -9.581e-02  3.736e-02  -2.564   0.0107 *  
## I(income^3)  6.855e-04  4.720e-04   1.452   0.1471    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.71 on 416 degrees of freedom
## Multiple R-squared:  0.5584, Adjusted R-squared:  0.5552 
## F-statistic: 175.4 on 3 and 416 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The estimated cubic model stored in <tt>cubic_model</tt> is</p>
<p><span class="math display">\[ \widehat{TestScore}_i = \underset{(5.83)}{600.1} + \underset{(0.86)}{5.02} \times income -\underset{(0.03)}{0.96} \times income^2 - \underset{(0.00047)}{0.00069} \times income^3. \]</span></p>
<p>From the output we can tell that the <span class="math inline">\(t\)</span>-statistic on <span class="math inline">\(income^3\)</span> is <span class="math inline">\(1.42\)</span> so the null that the relationship is quadratic cannot be rejected, even at the <span class="math inline">\(10\%\)</span> level. This is contrary to the result presented book which reports robust standard errors throughout so we will also use robust variance-covariance estimation to reproduce these results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load the lmtest package for coeftest()</span>
<span class="kw">library</span>(lmtest)

<span class="co"># test the hypothesis using robust standard errors</span>
<span class="kw">coeftest</span>(cubic_model, <span class="dt">vcov. =</span> <span class="kw">vcovHC</span>(cubic_model, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##                Estimate  Std. Error  t value  Pr(&gt;|t|)    
## (Intercept)  6.0008e+02  5.1021e+00 117.6150 &lt; 2.2e-16 ***
## income       5.0187e+00  7.0735e-01   7.0950 5.606e-12 ***
## I(income^2) -9.5805e-02  2.8954e-02  -3.3089  0.001018 ** 
## I(income^3)  6.8549e-04  3.4706e-04   1.9751  0.048918 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Notice that the reported standard errors have changed. Furthermore, the coefficient for <code>income^3</code> is now significant at the <span class="math inline">\(5\%\)</span> level. This means we reject the hypothesis that the regression function is quadratic against the alternative that it is cubic. Furthermore, we can also test if the coefficients for <tt>income^2</tt> and <tt>income^3</tt> are jointly significant using a robust version of the <span class="math inline">\(F\)</span>-test.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># robust F-test for </span>
<span class="kw">linearHypothesis</span>(cubic_model, 
                 <span class="dt">vcov. =</span> <span class="kw">vcovHC</span>(cubic_model, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>),
                 <span class="kw">c</span>(<span class="st">&quot;I(income^2)=0&quot;</span>, <span class="st">&quot;I(income^3)=0&quot;</span>)
                 )</code></pre></div>
<pre><code>## Linear hypothesis test
## 
## Hypothesis:
## I(income^2) = 0
## I(income^3) = 0
## 
## Model 1: restricted model
## Model 2: score ~ income + I(income^2) + I(income^3)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F    Pr(&gt;F)    
## 1    418                        
## 2    416  2 37.691 9.043e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>With a <span class="math inline">\(p\)</span>-value of <span class="math inline">\(9.043e^{-16}\)</span>, i.e. much less than <span class="math inline">\(0.05\)</span>, the null hypothesis of linearity is rejected in favor of the alternative that the relationship is quadratic <em>or</em> cubic.</p>
<div id="interpretation-of-coefficients-in-nonlinear-regression-models" class="section level4 unnumbered">
<h4>Interpretation of Coefficients in Nonlinear Regression Models</h4>
<p>The coefficients in polynomial regression models do not have a simple interpretation. Why is that? Think of a quadratic model: it is not helpful to think of the coefficient on <span class="math inline">\(X\)</span> as the expected change in <span class="math inline">\(Y\)</span> associated with a change in <span class="math inline">\(X\)</span> holding the other regressors constant because one other is <span class="math inline">\(X^2\)</span> which changes as <span class="math inline">\(X\)</span> is varied. This is also the case for other deviations from linearity, for example in models where regressors and/or the dependent variable are log-transformed. The best way to approach this is to calculate the estimated effect on <span class="math inline">\(Y\)</span> associated with a change in <span class="math inline">\(X\)</span> for one or more values of <span class="math inline">\(X\)</span>. This idea is summarized in Key Concept 8.1.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 8.1
</h3>
<h3 class="left">
The Expected Effect on <span class="math inline">\(Y\)</span> of a Change in <span class="math inline">\(X_1\)</span> in a Nonlinear Regression Model
</h3>
<p>Consider the nonlinear population regression model</p>
<p><span class="math display">\[ Y_i = f(X_{1i}, X_{2i}, \dots, X_{ki}) + u_i \ , \ i=1,\dots,n,\]</span></p>
<p>where <span class="math inline">\(f(X_{1i}, X_{2i}, \dots, X_{ki})\)</span> is the population regression function and <span class="math inline">\(u_i\)</span> is the error term.</p>
<p>Denote <span class="math inline">\(\Delta Y\)</span> the expected change in <span class="math inline">\(Y\)</span> associated with <span class="math inline">\(\Delta X_1\)</span>, the change in <span class="math inline">\(X_1\)</span> while holding <span class="math inline">\(X_2, \cdots , X_k\)</span> constant. That is, the expected change in <span class="math inline">\(Y\)</span> is the difference</p>
<p><span class="math display">\[\Delta Y = f(X_1 + \Delta X_1, X_2, \cdots, X_k) - f(X_1, X_2, \cdots, X_k).\]</span></p>
<p>The estimator of this unknown population difference is the difference between the predicted values for these two cases. Let <span class="math inline">\(\hat{f}(X_1, X_2, \cdots, X_k)\)</span> be the predicted value of of <span class="math inline">\(Y\)</span> based on the estimator <span class="math inline">\(\hat{f}\)</span> of the population regression function. Then the predicted change in <span class="math inline">\(Y\)</span> is</p>
<span class="math display">\[\Delta \widehat{Y} = \hat{f}(X_1 + \Delta X_1, X_2, \cdots, X_k) - \hat{f}(X_1, X_2, \cdots, X_k).\]</span>
</p>
</div>
<p>For example, we may ask the following: what is the predicted change in test scores associated with a one unit change (i.e. <span class="math inline">\(\$1000\)</span>) in income, based on the estimated quadratic regression function</p>
<p><span class="math display">\[\widehat{TestScore} = 607.3 + 3.85 \times income - 0.0423 \times income^2\ ?\]</span></p>
<p>Because the regression function is quadratic, this effect depends on the initial district income. We therefore consider two cases:</p>
<ol style="list-style-type: decimal">
<li><p>An increase in district income form <span class="math inline">\(10\)</span> to <span class="math inline">\(11\)</span> (i.e. from <span class="math inline">\(\$10000\)</span> per capita to <span class="math inline">\(\$11000\)</span>)</p></li>
<li><p>An increase in district income from <span class="math inline">\(40\)</span> to <span class="math inline">\(41\)</span> (that is from <span class="math inline">\(\$40000\)</span> to <span class="math inline">\(\$41000\)</span>).</p></li>
</ol>
<p>In order to obtain the <span class="math inline">\(\Delta \widehat{Y}\)</span> associated with a change in income form <span class="math inline">\(10\)</span> to <span class="math inline">\(11\)</span>, we use the following formula:</p>
<p><span class="math display">\[\Delta \widehat{Y} = \left(\hat{\beta}_0 + \hat{\beta}_1 \times 11 + \hat{\beta}_2 \times 11^2\right) - \left(\hat{\beta}_0 + \hat{\beta}_1 \times 10 + \hat{\beta}_2 \times 10^2\right) \]</span> To compute <span class="math inline">\(\widehat{Y}\)</span> using <tt>R</tt> we may use <tt>predict()</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute and assign the quadratic model</span>
quadriatic_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>income <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(income<span class="op">^</span><span class="dv">2</span>), <span class="dt">data =</span> CASchools)

<span class="co"># set up data to predict</span>
new_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">income =</span> <span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">11</span>))

<span class="co"># do the prediction</span>
Y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(quadriatic_model, <span class="dt">newdata =</span> new_data)

<span class="co"># compute the difference</span>
<span class="kw">diff</span>(Y_hat)</code></pre></div>
<pre><code>##        2 
## 2.962517</code></pre>
<p>Analogously we can compute the effect of a change in district income from <span class="math inline">\(40\)</span> to <span class="math inline">\(41\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set up data to predict</span>
new_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">income =</span> <span class="kw">c</span>(<span class="dv">40</span>, <span class="dv">41</span>))

<span class="co"># do the prediction</span>
Y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(quadriatic_model, <span class="dt">newdata =</span> new_data)

<span class="co"># compute the difference</span>
<span class="kw">diff</span>(Y_hat)</code></pre></div>
<pre><code>##         2 
## 0.4240097</code></pre>
<p>So for the quadratic model, the expected change in <span class="math inline">\(TestScore\)</span> induced by an increase in <span class="math inline">\(income\)</span> from <span class="math inline">\(10\)</span> to <span class="math inline">\(11\)</span> is about <span class="math inline">\(2.96\)</span> points but an increase in <span class="math inline">\(income\)</span> from <span class="math inline">\(40\)</span> to <span class="math inline">\(41\)</span> increases the predicted score by only <span class="math inline">\(0.42\)</span>. Hence the slope of the estimated quadratic regression function is <em>steeper</em> at low levels of income than at higher levels.</p>
</div>
</div>
<div id="logarithms" class="section level3 unnumbered">
<h3>Logarithms</h3>
<p>Another way to specify a nonlinear regression function is to use the natural logarithm of <span class="math inline">\(Y\)</span> and/or <span class="math inline">\(X\)</span>. Logarithms convert changes in variables into percentage changes which is convenient as many relationships are naturally expressed in terms of percentages.</p>
<p>There are three different cases in which logarithms might be used.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(X\)</span> could be transformed by taking its logarithm but <span class="math inline">\(Y\)</span> is not.</p></li>
<li><p>We could transform <span class="math inline">\(Y\)</span> to its logarithm but leave <span class="math inline">\(X\)</span> at level.</p></li>
<li><p>A third case is that both <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are transformed to their logarithms.</p></li>
</ol>
<p>The interpretation of the regression coefficients is different in each case.</p>
<div id="case-i-x-is-in-logarithm-y-is-not." class="section level4 unnumbered">
<h4>Case I: <span class="math inline">\(X\)</span> is in Logarithm, <span class="math inline">\(Y\)</span> is not.</h4>
<p>The regression model then is</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 \times \ln(X_i) + u_i \text{, } i=1,...,n. \]</span> Similar as for polynomial regression we do not have to create a new variable by computing <span class="math inline">\(\ln(X)\)</span>. We can simply adjust the <tt>formula</tt> argument of <tt>lm()</tt> to tell <tt>R</tt> that the log-transformation of a variable should be used.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate a level-log model</span>
LinearLog_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(income), <span class="dt">data =</span> CASchools)

<span class="co"># compute robust summary</span>
<span class="kw">coeftest</span>(LinearLog_model, 
         <span class="dt">vcov =</span> <span class="kw">vcovHC</span>(LinearLog_model, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)
         )</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##             Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 557.8323     3.8399 145.271 &lt; 2.2e-16 ***
## log(income)  36.4197     1.3969  26.071 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>According to the output the estimated regression function is:</p>
<p><span class="math display">\[\widehat{TestScore} = 557.8 + 36.42 \times \ln(income).\]</span></p>
<p>Let us draw a plot of this function.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># draw scatterplot</span>
<span class="kw">plot</span>(score <span class="op">~</span><span class="st"> </span>income, 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">data =</span> CASchools,
     <span class="dt">main =</span> <span class="st">&quot;Linear-Log Regression Line&quot;</span>)

<span class="co"># add Linear-Log regression line</span>
order_id  &lt;-<span class="st"> </span><span class="kw">order</span>(CASchools<span class="op">$</span>income)

<span class="kw">lines</span>(CASchools<span class="op">$</span>income[order_id],
      <span class="kw">fitted</span>(LinearLog_model)[order_id], 
      <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, 
      <span class="dt">lwd =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-320-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>We can interpret <span class="math inline">\(\hat{\beta}_1\)</span> as follows: a <span class="math inline">\(1\%\)</span> increase in income is associated with an increase in test scores of <span class="math inline">\(0.01 \times 36.42 = 0.36\)</span> points. In order to get the estimated effect of <it>a one unit </it> change in income (that is a change in the original units, thousands of dollars) on test scores, the method presented in Key Concept 8.1 can be used.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set up new data</span>
new_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">income =</span> <span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">11</span>, <span class="dv">40</span>, <span class="dv">41</span>))

<span class="co"># predict outcomes </span>
Y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(LinearLog_model, <span class="dt">newdata =</span> new_data)

<span class="co"># compute expected difference</span>
changes &lt;-<span class="st"> </span><span class="kw">matrix</span>(Y_hat, <span class="dt">nrow =</span> <span class="dv">2</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
changes[, <span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>changes[, <span class="dv">1</span>]</code></pre></div>
<pre><code>## [1] 3.471166 0.899297</code></pre>
<p>The estimated model states that for an income increase from <span class="math inline">\(\$10000\)</span> to <span class="math inline">\(\$11000\)</span>, test scores increase by an expected amount of <span class="math inline">\(3.47\)</span> points. When income increases from <span class="math inline">\(\$40000\)</span> to <span class="math inline">\(\$41000\)</span>, the expected increase in test scores is only about <span class="math inline">\(0.90\)</span> points.</p>
</div>
<div id="case-ii-y-is-in-logarithm-x-is-not" class="section level4 unnumbered">
<h4>Case II: <span class="math inline">\(Y\)</span> is in Logarithm, <span class="math inline">\(X\)</span> is not</h4>
<p>If you want to learn about the absolute impact of an explanatory variable on the dependent variable, it is not recommended to log-transform the latter. There are, however, cases where we want to learn about <span class="math inline">\(\ln(Y)\)</span> instead of <span class="math inline">\(Y\)</span>.</p>
<p>The corresponding regression model then is</p>
<p><span class="math display">\[ \ln(Y_i) = \beta_0 + \beta_1 \times X_i + u_i \ \ , \ \ i=1,...,n. \]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate a log-linear model </span>
LogLinear_model &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(score) <span class="op">~</span><span class="st"> </span>income, <span class="dt">data =</span> CASchools)

<span class="co"># compute a robust summary</span>
<span class="kw">coeftest</span>(LogLinear_model, 
         <span class="dt">vcov =</span> <span class="kw">vcovHC</span>(LogLinear_model, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)
         )</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##               Estimate Std. Error  t value  Pr(&gt;|t|)    
## (Intercept) 6.43936234 0.00289382 2225.210 &lt; 2.2e-16 ***
## income      0.00284407 0.00017509   16.244 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The estimated regression function is <span class="math display">\[\widehat{\ln(TestScore)} = 6.439 + 0.00284 \times income.\]</span> Since we are interested in <span class="math inline">\(\ln(Y)\)</span> rather than <span class="math inline">\(Y\)</span>, we <em>do not</em> re-transform the dependent variable.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># scatterplot</span>
<span class="kw">plot</span>(<span class="kw">log</span>(score) <span class="op">~</span><span class="st"> </span>income, 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">pch =</span> <span class="dv">20</span>, 
     <span class="dt">data =</span> CASchools,
     <span class="dt">main =</span> <span class="st">&quot;Log-Linear Regression Function&quot;</span>
     )

<span class="co"># add the Log-Linear regression line</span>
order_id  &lt;-<span class="st"> </span><span class="kw">order</span>(CASchools<span class="op">$</span>income)

<span class="kw">lines</span>(CASchools<span class="op">$</span>income[order_id], 
      <span class="kw">fitted</span>(LogLinear_model)[order_id], 
      <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, 
      <span class="dt">lwd =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-323-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>Note that the <span class="math inline">\(Y\)</span>-Axis is now log-transformed.</p>
<p>In a log-linear model, a one-unit change in <span class="math inline">\(X\)</span> is associated with an estimated <span class="math inline">\(100 \times \hat\beta_1 \%\)</span> change in <span class="math inline">\(Y\)</span>. This time we leave the <span class="math inline">\(X\)</span> values unchanged.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># do the predictions</span>
Y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(LogLinear_model, <span class="dt">newdata =</span> new_data)

<span class="co"># compute difference in changes</span>
changes &lt;-<span class="st"> </span><span class="kw">matrix</span>(Y_hat, <span class="dt">nrow =</span> <span class="dv">2</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
changes[, <span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>changes[, <span class="dv">1</span>]</code></pre></div>
<pre><code>## [1] 0.00284407 0.00284407</code></pre>
</div>
<div id="case-iii-x-and-y-are-in-logarithms" class="section level4 unnumbered">
<h4>Case III: <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are in Logarithms</h4>
<p>The log-log regression model is</p>
<p><span class="math display">\[\ln(Y_i) = \beta_0 + \beta_1 \times \ln(X_i) + u_i \ \ , \ \ i=1,...,n. \]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Estimate the Log-Log model</span>
LogLog_model &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(score) <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(income), <span class="dt">data =</span> CASchools)

<span class="co"># print robust summary to the console</span>
<span class="kw">coeftest</span>(LogLog_model, 
         <span class="dt">vcov =</span> <span class="kw">vcovHC</span>(LogLog_model, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)
         )</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##              Estimate Std. Error  t value  Pr(&gt;|t|)    
## (Intercept) 6.3363494  0.0059246 1069.501 &lt; 2.2e-16 ***
## log(income) 0.0554190  0.0021446   25.841 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The estimated regression function hence is <span class="math display">\[\widehat{\ln(TestScore)} = 6.336 + 0.0554 \times \ln(income).\]</span></p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># scatterplot</span>
<span class="kw">plot</span>(<span class="kw">log</span>(score) <span class="op">~</span><span class="st"> </span>income, 
     <span class="dt">data =</span> CASchools,
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">main =</span> <span class="st">&quot;Log-Log Regression Function&quot;</span>)

<span class="co"># plot the estimate log-log regression line</span>
<span class="kw">lines</span>(<span class="kw">sort</span>(CASchools<span class="op">$</span>income), 
      <span class="kw">fitted</span>(LogLog_model)[<span class="kw">order</span>(CASchools<span class="op">$</span>income)], 
      <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, 
      <span class="dt">lwd =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-325-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>In a log-log model, a <span class="math inline">\(1\%\)</span> change in <span class="math inline">\(X\)</span> is associated with an estimated <span class="math inline">\(\hat\beta_1 \%\)</span> change in <span class="math inline">\(Y\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># predict Y</span>
Y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(LogLog_model, <span class="dt">newdata =</span> new_data)

<span class="co"># compute changes</span>
changes &lt;-<span class="st"> </span><span class="kw">matrix</span>(Y_hat, <span class="dt">nrow =</span> <span class="dv">2</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
changes[ ,<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>changes[ ,<span class="dv">1</span>]</code></pre></div>
<pre><code>## [1] 0.005281992 0.001368439</code></pre>
<p>Key Concept 8.2 summarizes the three logarithmic regression models.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 8.2
</h3>
<h3 class="left">
Logarithms in Regression: Three Cases
</h3>
<p>
<p>Logarithms can be used to transform the dependent variable <span class="math inline">\(Y\)</span> or the independent variable <span class="math inline">\(X\)</span>, or both (the variable being transformed must be positive). The following table summarizes these three cases and the interpretation of the regression coefficient <span class="math inline">\(\beta_1\)</span>. In each case, <span class="math inline">\(\beta_1\)</span>, can be estimated by applying OLS after taking the logarithm(s) of the dependent and/or the independent variable.</p>
<table>
<thead>
<tr class="header">
<th align="left">
Case
</th>
<th align="left">
Model Specification
</th>
<th align="left">
Interpretation of <span class="math inline">\(\beta_1\)</span>
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">
<span class="math inline">\((I)\)</span>
</td>
<td align="left">
<span class="math inline">\(Y_i = \beta_0 + \beta_1 \ln(X_i) + u_i\)</span>
</td>
<td align="left">
A <span class="math inline">\(1 \%\)</span> change in <span class="math inline">\(X\)</span> is associated with a change in <span class="math inline">\(Y\)</span> of <span class="math inline">\(0.01 \times \beta_1\)</span>.
</td>
</tr>
<tr class="even">
<td align="left">
<span class="math inline">\((II)\)</span>
</td>
<td align="left">
<span class="math inline">\(\ln(Y_i) = \beta_0 + \beta_1 X_i + u_i\)</span>
</td>
<td align="left">
A change in <span class="math inline">\(X\)</span> by one unit (<span class="math inline">\(\Delta X = 1\)</span>) is associated with a <span class="math inline">\(100 \times \beta_1 \%\)</span> change in <span class="math inline">\(Y\)</span>.
</td>
</tr>
<tr class="odd">
<td align="left">
<span class="math inline">\((III)\)</span>
</td>
<td align="left">
<span class="math inline">\(\ln(Y_i) = \beta_0 + \beta_1 \ln(X_i) + u_i\)</span>
</td>
<td align="left">
A <span class="math inline">\(1\%\)</span> change in <span class="math inline">\(X\)</span> is associated with a <span class="math inline">\(\beta_1\%\)</span> change in <span class="math inline">\(Y\)</span>, so <span class="math inline">\(\beta_1\)</span> is the elasticity of <span class="math inline">\(Y\)</span> with respect to <span class="math inline">\(X\)</span>.
</td>
</tr>
</tbody>
</table>
</p>
</div>
<p>Of course we can also estimate a <it>polylog</it> model like</p>
<p><span class="math display">\[ TestScore_i = \beta_0 + \beta_1 \times \ln(income_i) + \beta_2 \times \ln(income_i)^2 + \beta_3 \times \ln(income_i)^3 + u_i \]</span></p>
<p>which models the dependent variable <span class="math inline">\(TestScore\)</span> by a third-degree polynomial of the log-transformed regressor <span class="math inline">\(income\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate the polylog model</span>
polyLog_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(income) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(income)<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(income)<span class="op">^</span><span class="dv">3</span>), 
                    <span class="dt">data =</span> CASchools)

<span class="co"># print robust summary to the console</span>
<span class="kw">coeftest</span>(polyLog_model, 
         <span class="dt">vcov =</span> <span class="kw">vcovHC</span>(polyLog_model, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##                  Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept)      486.1341    79.3825  6.1239 2.115e-09 ***
## log(income)      113.3820    87.8837  1.2901    0.1977    
## I(log(income)^2) -26.9111    31.7457 -0.8477    0.3971    
## I(log(income)^3)   3.0632     3.7369  0.8197    0.4128    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Which of the models presented here is the most suitable one? Comparing by <span class="math inline">\(\overline{R^2}\)</span> we find that, leaving out the log-linear model, all models have a similar fit. In the class of polynomial models, the cubic specification has the highest <span class="math inline">\(\overline{R^2}\)</span> whereas the linear-log specification is the best of the log-models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the adj. R^2 for the nonlinear models</span>
R2 &lt;-<span class="kw">rbind</span>(<span class="st">&quot;quadratic&quot;</span> =<span class="st"> </span><span class="kw">summary</span>(quadratic_model)<span class="op">$</span>adj.r.squared,
           <span class="st">&quot;cubic&quot;</span> =<span class="st"> </span><span class="kw">summary</span>(cubic_model)<span class="op">$</span>adj.r.squared,
           <span class="st">&quot;LinearLog&quot;</span> =<span class="st"> </span><span class="kw">summary</span>(LinearLog_model)<span class="op">$</span>adj.r.squared,
           <span class="st">&quot;LogLinear&quot;</span> =<span class="st"> </span><span class="kw">summary</span>(LogLinear_model)<span class="op">$</span>adj.r.squared,
           <span class="st">&quot;LogLog&quot;</span> =<span class="st"> </span><span class="kw">summary</span>(LogLog_model)<span class="op">$</span>adj.r.squared,
           <span class="st">&quot;polyLog&quot;</span> =<span class="st"> </span><span class="kw">summary</span>(polyLog_model)<span class="op">$</span>adj.r.squared
)
<span class="kw">colnames</span>(R2) &lt;-<span class="st"> &quot;R^2&quot;</span>
R2</code></pre></div>
<pre><code>##                 R^2
## quadratic 0.5540444
## cubic     0.5552279
## LinearLog 0.5614605
## LogLinear 0.4970106
## LogLog    0.5567251
## polyLog   0.5599944</code></pre>
<p>Let us now compare the cubic model and the linear-log model graphically by plotting the corresponding estimated regression functions.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># scatter plot</span>
<span class="kw">plot</span>(score <span class="op">~</span><span class="st"> </span>income, 
     <span class="dt">data =</span> CASchools,
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">main =</span> <span class="st">&quot;Linear-Log and Cubic Regression Functions&quot;</span>)

<span class="co"># add Linear-Log regression line</span>
order_id  &lt;-<span class="st"> </span><span class="kw">order</span>(CASchools<span class="op">$</span>income)

<span class="kw">lines</span>(CASchools<span class="op">$</span>income[order_id],
      <span class="kw">fitted</span>(LinearLog_model)[order_id], 
      <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span>, 
      <span class="dt">lwd =</span> <span class="dv">2</span>)

<span class="co"># add cubic regression line</span>
<span class="kw">lines</span>(<span class="dt">x =</span> CASchools<span class="op">$</span>income[order_id], 
      <span class="dt">y =</span> <span class="kw">fitted</span>(cubic_model)[order_id],
      <span class="dt">col=</span><span class="st">&quot;darkred&quot;</span>, 
      <span class="dt">lwd=</span><span class="dv">2</span>) </code></pre></div>
<p><img src="URFITE_files/figure-html/unnamed-chunk-330-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>We see that both regression lines look nearly identical. Altogether The linear-log model is preferable since it has a slightly higher <span class="math inline">\(\overline{R^2}\)</span> and is more parsimonious in terms of regressors: it does not include higher-degree polynomials.</p>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-stock2015">
<p>Stock, J., &amp; Watson, M. (2015). <em>Introduction to econometrics, third update, global edition</em>. Pearson Education Limited.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="a-general-strategy-for-modelling-nonlinear-regression-functions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="interactions-between-independent-variables.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 1
},
"edit": null,
"download": null,
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
